{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kornia --upgrade\n",
    "#!pip install pytorch_metric_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import random\n",
    "import numpy as np\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from fastai2.basics import *\n",
    "from fastcore import *\n",
    "from fastai2.vision.all import *\n",
    "from fastai2.callback.all import *\n",
    "from fastprogress import fastprogress\n",
    "from fastai2.callback.mixup import *\n",
    "from fastscript import *\n",
    "import torchvision as tv\n",
    "import kornia as K\n",
    "import gc\n",
    "from pytorch_metric_learning import losses, miners\n",
    "\n",
    "def imshow_torch(tensor, *kwargs):\n",
    "    plt.figure()\n",
    "    plt.imshow(K.tensor_to_image(tensor), *kwargs)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_name = 'liberty'\n",
    "val_ds_names = ['notredame', 'yosemite']\n",
    "\n",
    "ds_root = '/home/old-ufo/datasets/Brown/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import Any, Callable, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import VisionDataset\n",
    "\n",
    "from torchvision.datasets.utils import download_url\n",
    "\n",
    "from fastai2  import *\n",
    "\n",
    "\n",
    "class PhotoTourRevisited(torchvision.datasets.VisionDataset):\n",
    "    \"\"\"`Learning Local Image Descriptors Data <http://phototour.cs.washington.edu/patches/default.htm>`_ Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory where images are.\n",
    "        name (string): Name of the dataset to load.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "    \"\"\"\n",
    "    urls = {\n",
    "        'notredame_harris': [\n",
    "            'http://matthewalunbrown.com/patchdata/notredame_harris.zip',\n",
    "            'notredame_harris.zip',\n",
    "            '69f8c90f78e171349abdf0307afefe4d'\n",
    "        ],\n",
    "        'yosemite_harris': [\n",
    "            'http://matthewalunbrown.com/patchdata/yosemite_harris.zip',\n",
    "            'yosemite_harris.zip',\n",
    "            'a73253d1c6fbd3ba2613c45065c00d46'\n",
    "        ],\n",
    "        'liberty_harris': [\n",
    "            'http://matthewalunbrown.com/patchdata/liberty_harris.zip',\n",
    "            'liberty_harris.zip',\n",
    "            'c731fcfb3abb4091110d0ae8c7ba182c'\n",
    "        ],\n",
    "        'notredame': [\n",
    "            'http://icvl.ee.ic.ac.uk/vbalnt/notredame.zip',\n",
    "            'notredame.zip',\n",
    "            '509eda8535847b8c0a90bbb210c83484'\n",
    "        ],\n",
    "        'yosemite': [\n",
    "            'http://icvl.ee.ic.ac.uk/vbalnt/yosemite.zip',\n",
    "            'yosemite.zip',\n",
    "            '533b2e8eb7ede31be40abc317b2fd4f0'\n",
    "        ],\n",
    "        'liberty': [\n",
    "            'http://icvl.ee.ic.ac.uk/vbalnt/liberty.zip',\n",
    "            'liberty.zip',\n",
    "            'fdd9152f138ea5ef2091746689176414'\n",
    "        ],\n",
    "    }\n",
    "    means = {'notredame': 0.4854, 'yosemite': 0.4844, 'liberty': 0.4437,\n",
    "             'notredame_harris': 0.4854, 'yosemite_harris': 0.4844, 'liberty_harris': 0.4437}\n",
    "    stds = {'notredame': 0.1864, 'yosemite': 0.1818, 'liberty': 0.2019,\n",
    "            'notredame_harris': 0.1864, 'yosemite_harris': 0.1818, 'liberty_harris': 0.2019}\n",
    "    lens = {'notredame': 468159, 'yosemite': 633587, 'liberty': 450092,\n",
    "            'liberty_harris': 379587, 'yosemite_harris': 450912, 'notredame_harris': 325295}\n",
    "    image_ext = 'bmp'\n",
    "    info_file = 'info.txt'\n",
    "    matches_files = 'm50_100000_100000_0.txt'\n",
    "    img_info_files = 'interest.txt'\n",
    "\n",
    "    def __init__(\n",
    "            self, root: str, name: str, train: bool = False,\n",
    "        transform: Optional[Callable] = None, download: bool = False\n",
    "    ) -> None:\n",
    "        super(PhotoTourRevisited, self).__init__(root)\n",
    "        self.name = name\n",
    "        self.data_dir = os.path.join(self.root, name)\n",
    "        self.data_down = os.path.join(self.root, '{}.zip'.format(name))\n",
    "        self.data_file = os.path.join(self.root, '{}.pt'.format(name))\n",
    "\n",
    "        self.train = train\n",
    "        self.mean = self.means[name]\n",
    "        self.std = self.stds[name]\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_datafile_exists():\n",
    "            raise RuntimeError('Dataset not found.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        # load the serialized data\n",
    "        self.data, self.labels, self.matches, self.img_idxs = torch.load(self.data_file)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Union[torch.Tensor, Tuple[Any, Any, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (data1, data2, matches)\n",
    "        \"\"\"\n",
    "        data = self.data[index]\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        if self.train:\n",
    "            return data\n",
    "        return data, self.labels[index], self.img_idxs[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.lens[self.name]\n",
    "\n",
    "\n",
    "    def _check_datafile_exists(self) -> bool:\n",
    "        return os.path.exists(self.data_file)\n",
    "\n",
    "    def _check_downloaded(self) -> bool:\n",
    "        return os.path.exists(self.data_dir)\n",
    "\n",
    "    def download(self) -> None:\n",
    "        if self._check_datafile_exists():\n",
    "            print('# Found cached data {}'.format(self.data_file))\n",
    "            return\n",
    "\n",
    "        if not self._check_downloaded():\n",
    "            # download files\n",
    "            url = self.urls[self.name][0]\n",
    "            filename = self.urls[self.name][1]\n",
    "            md5 = self.urls[self.name][2]\n",
    "            fpath = os.path.join(self.root, filename)\n",
    "\n",
    "            download_url(url, self.root, filename, md5)\n",
    "\n",
    "            print('# Extracting data {}\\n'.format(self.data_down))\n",
    "\n",
    "            import zipfile\n",
    "            with zipfile.ZipFile(fpath, 'r') as z:\n",
    "                z.extractall(self.data_dir)\n",
    "\n",
    "            os.unlink(fpath)\n",
    "\n",
    "        # process and save as torch files\n",
    "        print('# Caching data {}'.format(self.data_file))\n",
    "\n",
    "        dataset = (\n",
    "            read_image_file(self.data_dir, self.image_ext, self.lens[self.name]),\n",
    "            read_info_file(self.data_dir, self.info_file),\n",
    "            read_matches_files(self.data_dir, self.matches_files),\n",
    "            read_interest_file(self.data_dir, self.img_info_files)\n",
    "        )\n",
    "\n",
    "        with open(self.data_file, 'wb') as f:\n",
    "            torch.save(dataset, f)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return \"Split: {}\".format(\"Train\" if self.train is True else \"Test\")\n",
    "\n",
    "\n",
    "def read_image_file(data_dir: str, image_ext: str, n: int) -> torch.Tensor:\n",
    "    \"\"\"Return a Tensor containing the patches\n",
    "    \"\"\"\n",
    "\n",
    "    def PIL2array(_img: Image.Image) -> np.ndarray:\n",
    "        \"\"\"Convert PIL image type to numpy 2D array\n",
    "        \"\"\"\n",
    "        return np.array(_img.getdata(), dtype=np.uint8).reshape(64, 64)\n",
    "\n",
    "    def find_files(_data_dir: str, _image_ext: str) -> List[str]:\n",
    "        \"\"\"Return a list with the file names of the images containing the patches\n",
    "        \"\"\"\n",
    "        files = []\n",
    "        # find those files with the specified extension\n",
    "        for file_dir in os.listdir(_data_dir):\n",
    "            if file_dir.endswith(_image_ext):\n",
    "                files.append(os.path.join(_data_dir, file_dir))\n",
    "        return sorted(files)  # sort files in ascend order to keep relations\n",
    "\n",
    "    patches = []\n",
    "    list_files = find_files(data_dir, image_ext)\n",
    "\n",
    "    for fpath in list_files:\n",
    "        img = Image.open(fpath)\n",
    "        for y in range(0, 1024, 64):\n",
    "            for x in range(0, 1024, 64):\n",
    "                patch = img.crop((x, y, x + 64, y + 64))\n",
    "                patches.append(PIL2array(patch))\n",
    "    return torch.ByteTensor(np.array(patches[:n]))#.float()\n",
    "\n",
    "\n",
    "def read_info_file(data_dir: str, info_file: str) -> torch.Tensor:\n",
    "    \"\"\"Return a Tensor containing the list of labels\n",
    "       Read the file and keep only the ID of the 3D point.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    with open(os.path.join(data_dir, info_file), 'r') as f:\n",
    "        labels = [int(line.split()[0]) for line in f]\n",
    "    return torch.LongTensor(labels)\n",
    "\n",
    "def read_interest_file(data_dir: str, info_file: str) -> torch.Tensor:\n",
    "    \"\"\"Return a Tensor containing the list of image ids\n",
    "       Read the file and keep only the ID of the image point.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    with open(os.path.join(data_dir, info_file), 'r') as f:\n",
    "        labels = [int(line.split()[0]) for line in f]\n",
    "    return torch.LongTensor(labels)\n",
    "\n",
    "\n",
    "def read_matches_files(data_dir: str, matches_file: str) -> torch.Tensor:\n",
    "    \"\"\"Return a Tensor containing the ground truth matches\n",
    "       Read the file and keep only 3D point ID.\n",
    "       Matches are represented with a 1, non matches with a 0.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    with open(os.path.join(data_dir, matches_file), 'r') as f:\n",
    "        for line in f:\n",
    "            line_split = line.split()\n",
    "            matches.append([int(line_split[0]), int(line_split[3]),\n",
    "                            int(line_split[1] == line_split[4])])\n",
    "    return torch.LongTensor(matches)\n",
    "\n",
    "class TupleAug(ItemTransform):\n",
    "    def __init__(self, tfm):\n",
    "        self.tfm = tfm\n",
    "    def encodes(self, o): \n",
    "        out = []\n",
    "        with torch.no_grad():\n",
    "            for i,oi in enumerate(o):\n",
    "                if i < len(o) - 2:\n",
    "                    out.append(self.tfm(oi.float().unsqueeze(1)))\n",
    "                else:\n",
    "                    out.append(oi)\n",
    "        return out\n",
    "#def average_acc_per_th(snn_ratio, is_correct, ths= np.linspace(0,1.0,20) ):\n",
    "#    out = []\n",
    "#    for prev_th, th in zip(ths[:-1], ths[1:]):\n",
    "#        mask = snn_ratio <= th\n",
    "#        #print (mask.sum())\n",
    "#        AA = is_correct[mask].float().mean()\n",
    "#        #print (mask.sum().item(), AA.item())\n",
    "#        out.append(AA.item())\n",
    "#    return out\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_descriptor_on_dataset( desc,\n",
    "    ds_name='notredame',\n",
    "    resol=100,\n",
    "    device=torch.device('cuda:0'),\n",
    "    ds_root = '/home/old-ufo/datasets/Brown'):\n",
    "    \n",
    "    desc.eval()\n",
    "    desc = desc.to(device)\n",
    "    dataset = PhotoTourRevisited(ds_root,\n",
    "                      ds_name,\n",
    "                       train=False, \n",
    "                       download=True)\n",
    "    orig_size = 64\n",
    "    out_size = 32\n",
    "    test_aug = nn.Sequential( \n",
    "        K.Resize((out_size,out_size), interpolation='bicubic'))\n",
    "    BS = 1024\n",
    "    TEST_BS = 128\n",
    "    N_WORKERS = 4\n",
    "    \n",
    "    dl_train = TfmdDL(dataset,\n",
    "                 device=device,\n",
    "                 after_item=[ToTensor], \n",
    "                 after_batch=[TupleAug(test_aug)], #two patches -> single tensor\n",
    "                 bs=BS, num_workers=N_WORKERS,\n",
    "                 shuffle = False)\n",
    "    num_patches = len(dl_train.dataset)\n",
    "    descriptors = []#torch.zeros(num_patches, 128)\n",
    "    all_labels = []#torch.zeros(num_patches)\n",
    "    all_img_labels = []#torch.zeros(num_patches)\n",
    "    Miner = miners.BatchHardMiner()\n",
    "    count = 0 \n",
    "    max_img = -1\n",
    "    min_img = 1000\n",
    "    prec_per_img = []\n",
    "    rec_per_img = []\n",
    "    ths_per_img = []\n",
    "    aps = []\n",
    "    print ('Extracting descriptors and calculating AP')\n",
    "    for patches, labels, img_labels in progress_bar(dl_train):\n",
    "        with torch.no_grad():\n",
    "            descs = desc(patches)\n",
    "            descriptors.append(descs)\n",
    "            all_labels.append(labels)\n",
    "            all_img_labels.append(img_labels)\n",
    "            all_img_labels_cat = torch.cat(all_img_labels)\n",
    "            img_labels_unique = torch.sort(torch.unique(all_img_labels_cat).long())[0]\n",
    "            new_max_img = img_labels_unique.max().item()\n",
    "            new_min_img = img_labels_unique.min().item()\n",
    "            if new_min_img != new_max_img:\n",
    "                all_img_labels = torch.cat(all_img_labels)\n",
    "                descriptors = torch.cat(descriptors)\n",
    "                all_labels = torch.cat(all_labels)\n",
    "                for ii in img_labels_unique[:-1]:\n",
    "                    current_batch = all_img_labels == ii\n",
    "                    cur_descs = descriptors[current_batch].cpu()\n",
    "                    cur_labels = all_labels[current_batch].cpu()\n",
    "                    anc, pos, neg = Miner(cur_descs, cur_labels)\n",
    "                    NN = cur_labels.size(0)\n",
    "                    pos_matrix = (cur_labels[None].expand(NN,NN) == cur_labels[...,None].expand(NN,NN)) != (torch.eye(NN).to(cur_labels.device)>0)\n",
    "                    pos_idxs = torch.arange(NN)[None].expand(NN,NN)[pos_matrix]\n",
    "                    anc_idxs = torch.nonzero(pos_matrix)[:,0]\n",
    "                    pos_matrix = None\n",
    "                    neg_idxs = neg[anc_idxs]\n",
    "                    pos_dists = F.pairwise_distance(cur_descs[anc_idxs], cur_descs[pos_idxs])\n",
    "                    neg_dists = F.pairwise_distance(cur_descs[anc_idxs], cur_descs[neg_idxs])\n",
    "                    correct = pos_dists <= neg_dists\n",
    "                    snn = torch.min(pos_dists,neg_dists) / torch.max(pos_dists,neg_dists)\n",
    "                    snn[torch.isnan(snn)] = 1.0\n",
    "                    #precision, recall, thresholds = precision_recall_curve(correct, 1-snn)\n",
    "                    ap = average_precision_score(correct, 1-snn)\n",
    "                    #prec_per_img.append(precision)\n",
    "                    #rec_per_img.append(recall)\n",
    "                    #ths_per_img.append(thresholds)\n",
    "                    aps.append(ap)\n",
    "                current_batch = all_img_labels == img_labels_unique[-1].item()\n",
    "                descriptors = [descriptors[current_batch]]\n",
    "                all_img_labels = [all_img_labels[current_batch]]\n",
    "                all_labels = [all_labels[current_batch]]\n",
    "                gc.collect()\n",
    "    all_img_labels = torch.cat(all_img_labels)\n",
    "    descriptors = torch.cat(descriptors)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    for ii in img_labels_unique:\n",
    "        current_batch = all_img_labels == ii\n",
    "        cur_descs = descriptors[current_batch].cpu()\n",
    "        cur_labels = all_labels[current_batch].cpu()\n",
    "        anc, pos, neg = Miner(cur_descs, cur_labels)\n",
    "        NN = cur_labels.size(0)\n",
    "        pos_matrix = (cur_labels[None].expand(NN,NN) == cur_labels[...,None].expand(NN,NN)) != (torch.eye(NN)>0)\n",
    "        pos_idxs = torch.arange(NN)[None].expand(NN,NN)[pos_matrix]\n",
    "        anc_idxs = torch.nonzero(pos_matrix)[:,0]\n",
    "        pos_matrix = None\n",
    "        neg_idxs = neg[anc_idxs]\n",
    "        pos_dists = F.pairwise_distance(cur_descs[anc_idxs], cur_descs[pos_idxs])\n",
    "        neg_dists = F.pairwise_distance(cur_descs[anc_idxs], cur_descs[neg_idxs])\n",
    "        correct = pos_dists <= neg_dists\n",
    "        snn = torch.min(pos_dists,neg_dists) / torch.max(pos_dists,neg_dists)\n",
    "        snn[torch.isnan(snn)] = 1.0\n",
    "        #precision, recall, thresholds = precision_recall_curve(correct, 1-snn)\n",
    "        #prec_per_img.append(precision)\n",
    "        #rec_per_img.append(recall)\n",
    "        #ths_per_img.append(thresholds)\n",
    "        ap = average_precision_score(correct, 1-snn)\n",
    "        aps.append(ap)\n",
    "    descriptors = None\n",
    "    all_labels = None\n",
    "    all_img_labels = None\n",
    "    dataset=None\n",
    "    dl_train = None\n",
    "    gc.collect()\n",
    "    return aps\n",
    "    #return {\"precision\": prec_per_img, \"recall\": rec_per_img, \"thresholds\": ths_per_img}\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "from collections import defaultdict\n",
    "results = defaultdict(dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yosemite\n",
      "RootSIFT\n",
      "# Found cached data /home/old-ufo/datasets/Brown/yosemite.pt\n",
      "Extracting descriptors and calculating AP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='619' class='' max='619' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [619/619 03:47<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RootSIFT mAP = 0.479\n",
      "SIFT\n",
      "# Found cached data /home/old-ufo/datasets/Brown/yosemite.pt\n",
      "Extracting descriptors and calculating AP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='619' class='' max='619' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [619/619 04:23<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIFT mAP = 0.487\n",
      "HardNet\n",
      "# Found cached data /home/old-ufo/datasets/Brown/yosemite.pt\n",
      "Extracting descriptors and calculating AP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='619' class='' max='619' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [619/619 04:49<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HardNet mAP = 0.658\n",
      "SoSNet\n",
      "# Found cached data /home/old-ufo/datasets/Brown/yosemite.pt\n",
      "Extracting descriptors and calculating AP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='67' class='' max='619' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      10.82% [67/619 00:46<06:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "results = defaultdict(dict)\n",
    "for ds_name in val_ds_names[::-1]:\n",
    "    print (ds_name)\n",
    "    for desc, desc_name in zip([\n",
    "         K.feature.SIFTDescriptor(32, rootsift=True).to(torch.device('cuda:0')),\n",
    "         K.feature.SIFTDescriptor(32, rootsift=False).to(torch.device('cuda:0')),\n",
    "        K.feature.HardNet(True),\n",
    "        K.feature.SOSNet(True)], ['RootSIFT', 'SIFT', 'HardNet', 'SoSNet']):\n",
    "        print (desc_name)\n",
    "        results[ds_name][desc_name] = eval_descriptor_on_dataset(desc,\n",
    "                                                                 ds_name, resol=50)\n",
    "        print (f'{desc_name} mAP = {np.array(results[ds_name][desc_name]).mean():.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
