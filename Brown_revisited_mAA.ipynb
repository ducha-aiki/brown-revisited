{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/kornia/kornia\n",
      "  Cloning https://github.com/kornia/kornia to /tmp/pip-req-build-mbr9urla\n",
      "  Running command git clone -q https://github.com/kornia/kornia /tmp/pip-req-build-mbr9urla\n",
      "Requirement already satisfied, skipping upgrade: numpy in /home/mishkdmy/.conda/envs/fastai1/lib/python3.7/site-packages (from kornia==0.4.1+0c9e625) (1.18.0)\n",
      "Requirement already satisfied, skipping upgrade: torch<1.7.0,>=1.6.0 in /home/mishkdmy/.local/lib/python3.7/site-packages (from kornia==0.4.1+0c9e625) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: future in /home/mishkdmy/.local/lib/python3.7/site-packages (from torch<1.7.0,>=1.6.0->kornia==0.4.1+0c9e625) (0.18.2)\n",
      "Building wheels for collected packages: kornia\n",
      "  Building wheel for kornia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kornia: filename=kornia-0.4.1+0c9e625-py2.py3-none-any.whl size=206171 sha256=08a2801e03669cb42247fa83bb6b4b996230e6ec62baaca632fb7bbb1c745184\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-yr06tdqf/wheels/5f/8b/92/375714dc479253f78ed777dd105d79f9693448dcfef85c8163\n",
      "Successfully built kornia\n",
      "Installing collected packages: kornia\n",
      "  Found existing installation: kornia 0.4.1+0c9e625\n",
      "    Uninstalling kornia-0.4.1+0c9e625:\n",
      "      Successfully uninstalled kornia-0.4.1+0c9e625\n",
      "Successfully installed kornia-0.4.1+0c9e625\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/kornia/kornia --user --upgrade\n",
    "#!pip install pytorch_metric_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastscript\n",
      "  Downloading https://files.pythonhosted.org/packages/60/e4/7790e3ca100841566fdc1ccee413b9a9d40629d1858d86b1b9ffbc4fa75a/fastscript-1.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: packaging in /home/mishkdmy/.conda/envs/fastai1/lib/python3.7/site-packages (from fastscript) (19.2)\n",
      "Requirement already satisfied: pip in /home/mishkdmy/.conda/envs/fastai1/lib/python3.7/site-packages (from fastscript) (19.3.1)\n",
      "Requirement already satisfied: six in /home/mishkdmy/.conda/envs/fastai1/lib/python3.7/site-packages (from packaging->fastscript) (1.13.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/mishkdmy/.conda/envs/fastai1/lib/python3.7/site-packages (from packaging->fastscript) (2.4.5)\n",
      "Installing collected packages: fastscript\n",
      "\u001b[33m  WARNING: The script test_fastscript is installed in '/home/mishkdmy/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed fastscript-1.0.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install fastscript --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import random\n",
    "import numpy as np\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from fastai2.basics import *\n",
    "from fastcore import *\n",
    "from fastai2.vision.all import *\n",
    "from fastai2.callback.all import *\n",
    "from fastprogress import fastprogress\n",
    "from fastai2.callback.mixup import *\n",
    "from fastscript import *\n",
    "import torchvision as tv\n",
    "import kornia as K\n",
    "import gc\n",
    "from pytorch_metric_learning import losses, miners\n",
    "\n",
    "def imshow_torch(tensor, *kwargs):\n",
    "    plt.figure()\n",
    "    plt.imshow(K.tensor_to_image(tensor), *kwargs)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_name = 'liberty'\n",
    "val_ds_names = ['notredame', 'yosemite']\n",
    "\n",
    "ds_root = '/home/mishkdmy/datasets/Brown/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import Any, Callable, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import VisionDataset\n",
    "\n",
    "from torchvision.datasets.utils import download_url\n",
    "\n",
    "from fastai2  import *\n",
    "\n",
    "\n",
    "class PhotoTourRevisited(torchvision.datasets.VisionDataset):\n",
    "    \"\"\"`Learning Local Image Descriptors Data <http://phototour.cs.washington.edu/patches/default.htm>`_ Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory where images are.\n",
    "        name (string): Name of the dataset to load.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "    \"\"\"\n",
    "    urls = {\n",
    "        'notredame_harris': [\n",
    "            'http://matthewalunbrown.com/patchdata/notredame_harris.zip',\n",
    "            'notredame_harris.zip',\n",
    "            '69f8c90f78e171349abdf0307afefe4d'\n",
    "        ],\n",
    "        'yosemite_harris': [\n",
    "            'http://matthewalunbrown.com/patchdata/yosemite_harris.zip',\n",
    "            'yosemite_harris.zip',\n",
    "            'a73253d1c6fbd3ba2613c45065c00d46'\n",
    "        ],\n",
    "        'liberty_harris': [\n",
    "            'http://matthewalunbrown.com/patchdata/liberty_harris.zip',\n",
    "            'liberty_harris.zip',\n",
    "            'c731fcfb3abb4091110d0ae8c7ba182c'\n",
    "        ],\n",
    "        'notredame': [\n",
    "            'http://icvl.ee.ic.ac.uk/vbalnt/notredame.zip',\n",
    "            'notredame.zip',\n",
    "            '509eda8535847b8c0a90bbb210c83484'\n",
    "        ],\n",
    "        'yosemite': [\n",
    "            'http://icvl.ee.ic.ac.uk/vbalnt/yosemite.zip',\n",
    "            'yosemite.zip',\n",
    "            '533b2e8eb7ede31be40abc317b2fd4f0'\n",
    "        ],\n",
    "        'liberty': [\n",
    "            'http://icvl.ee.ic.ac.uk/vbalnt/liberty.zip',\n",
    "            'liberty.zip',\n",
    "            'fdd9152f138ea5ef2091746689176414'\n",
    "        ],\n",
    "    }\n",
    "    means = {'notredame': 0.4854, 'yosemite': 0.4844, 'liberty': 0.4437,\n",
    "             'notredame_harris': 0.4854, 'yosemite_harris': 0.4844, 'liberty_harris': 0.4437}\n",
    "    stds = {'notredame': 0.1864, 'yosemite': 0.1818, 'liberty': 0.2019,\n",
    "            'notredame_harris': 0.1864, 'yosemite_harris': 0.1818, 'liberty_harris': 0.2019}\n",
    "    lens = {'notredame': 468159, 'yosemite': 633587, 'liberty': 450092,\n",
    "            'liberty_harris': 379587, 'yosemite_harris': 450912, 'notredame_harris': 325295}\n",
    "    image_ext = 'bmp'\n",
    "    info_file = 'info.txt'\n",
    "    matches_files = 'm50_100000_100000_0.txt'\n",
    "    img_info_files = 'interest.txt'\n",
    "\n",
    "    def __init__(\n",
    "            self, root: str, name: str, train: bool = False,\n",
    "        transform: Optional[Callable] = None, download: bool = False\n",
    "    ) -> None:\n",
    "        super(PhotoTourRevisited, self).__init__(root)\n",
    "        self.name = name\n",
    "        self.data_dir = os.path.join(self.root, name)\n",
    "        self.data_down = os.path.join(self.root, '{}.zip'.format(name))\n",
    "        self.data_file = os.path.join(self.root, '{}.pt'.format(name))\n",
    "\n",
    "        self.train = train\n",
    "        self.mean = self.means[name]\n",
    "        self.std = self.stds[name]\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_datafile_exists():\n",
    "            raise RuntimeError('Dataset not found.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        # load the serialized data\n",
    "        self.data, self.labels, self.matches, self.img_idxs = torch.load(self.data_file)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Union[torch.Tensor, Tuple[Any, Any, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (data1, data2, matches)\n",
    "        \"\"\"\n",
    "        data = self.data[index]\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        if self.train:\n",
    "            return data\n",
    "        return data, self.labels[index], self.img_idxs[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.lens[self.name]\n",
    "\n",
    "\n",
    "    def _check_datafile_exists(self) -> bool:\n",
    "        return os.path.exists(self.data_file)\n",
    "\n",
    "    def _check_downloaded(self) -> bool:\n",
    "        return os.path.exists(self.data_dir)\n",
    "\n",
    "    def download(self) -> None:\n",
    "        if self._check_datafile_exists():\n",
    "            print('# Found cached data {}'.format(self.data_file))\n",
    "            return\n",
    "\n",
    "        if not self._check_downloaded():\n",
    "            # download files\n",
    "            url = self.urls[self.name][0]\n",
    "            filename = self.urls[self.name][1]\n",
    "            md5 = self.urls[self.name][2]\n",
    "            fpath = os.path.join(self.root, filename)\n",
    "\n",
    "            download_url(url, self.root, filename, md5)\n",
    "\n",
    "            print('# Extracting data {}\\n'.format(self.data_down))\n",
    "\n",
    "            import zipfile\n",
    "            with zipfile.ZipFile(fpath, 'r') as z:\n",
    "                z.extractall(self.data_dir)\n",
    "\n",
    "            os.unlink(fpath)\n",
    "\n",
    "        # process and save as torch files\n",
    "        print('# Caching data {}'.format(self.data_file))\n",
    "\n",
    "        dataset = (\n",
    "            read_image_file(self.data_dir, self.image_ext, self.lens[self.name]),\n",
    "            read_info_file(self.data_dir, self.info_file),\n",
    "            read_matches_files(self.data_dir, self.matches_files),\n",
    "            read_interest_file(self.data_dir, self.img_info_files)\n",
    "        )\n",
    "\n",
    "        with open(self.data_file, 'wb') as f:\n",
    "            torch.save(dataset, f)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return \"Split: {}\".format(\"Train\" if self.train is True else \"Test\")\n",
    "\n",
    "\n",
    "def read_image_file(data_dir: str, image_ext: str, n: int) -> torch.Tensor:\n",
    "    \"\"\"Return a Tensor containing the patches\n",
    "    \"\"\"\n",
    "\n",
    "    def PIL2array(_img: Image.Image) -> np.ndarray:\n",
    "        \"\"\"Convert PIL image type to numpy 2D array\n",
    "        \"\"\"\n",
    "        return np.array(_img.getdata(), dtype=np.uint8).reshape(64, 64)\n",
    "\n",
    "    def find_files(_data_dir: str, _image_ext: str) -> List[str]:\n",
    "        \"\"\"Return a list with the file names of the images containing the patches\n",
    "        \"\"\"\n",
    "        files = []\n",
    "        # find those files with the specified extension\n",
    "        for file_dir in os.listdir(_data_dir):\n",
    "            if file_dir.endswith(_image_ext):\n",
    "                files.append(os.path.join(_data_dir, file_dir))\n",
    "        return sorted(files)  # sort files in ascend order to keep relations\n",
    "\n",
    "    patches = []\n",
    "    list_files = find_files(data_dir, image_ext)\n",
    "\n",
    "    for fpath in list_files:\n",
    "        img = Image.open(fpath)\n",
    "        for y in range(0, 1024, 64):\n",
    "            for x in range(0, 1024, 64):\n",
    "                patch = img.crop((x, y, x + 64, y + 64))\n",
    "                patches.append(PIL2array(patch))\n",
    "    return torch.ByteTensor(np.array(patches[:n]))#.float()\n",
    "\n",
    "\n",
    "def read_info_file(data_dir: str, info_file: str) -> torch.Tensor:\n",
    "    \"\"\"Return a Tensor containing the list of labels\n",
    "       Read the file and keep only the ID of the 3D point.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    with open(os.path.join(data_dir, info_file), 'r') as f:\n",
    "        labels = [int(line.split()[0]) for line in f]\n",
    "    return torch.LongTensor(labels)\n",
    "\n",
    "def read_interest_file(data_dir: str, info_file: str) -> torch.Tensor:\n",
    "    \"\"\"Return a Tensor containing the list of image ids\n",
    "       Read the file and keep only the ID of the image point.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    with open(os.path.join(data_dir, info_file), 'r') as f:\n",
    "        labels = [int(line.split()[0]) for line in f]\n",
    "    return torch.LongTensor(labels)\n",
    "\n",
    "\n",
    "def read_matches_files(data_dir: str, matches_file: str) -> torch.Tensor:\n",
    "    \"\"\"Return a Tensor containing the ground truth matches\n",
    "       Read the file and keep only 3D point ID.\n",
    "       Matches are represented with a 1, non matches with a 0.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    with open(os.path.join(data_dir, matches_file), 'r') as f:\n",
    "        for line in f:\n",
    "            line_split = line.split()\n",
    "            matches.append([int(line_split[0]), int(line_split[3]),\n",
    "                            int(line_split[1] == line_split[4])])\n",
    "    return torch.LongTensor(matches)\n",
    "\n",
    "class TupleAug(ItemTransform):\n",
    "    def __init__(self, tfm):\n",
    "        self.tfm = tfm\n",
    "    def encodes(self, o): \n",
    "        out = []\n",
    "        with torch.no_grad():\n",
    "            for i,oi in enumerate(o):\n",
    "                if i < len(o) - 2:\n",
    "                    out.append(self.tfm(oi.float().unsqueeze(1)))\n",
    "                else:\n",
    "                    out.append(oi)\n",
    "        return out\n",
    "#def average_acc_per_th(snn_ratio, is_correct, ths= np.linspace(0,1.0,20) ):\n",
    "#    out = []\n",
    "#    for prev_th, th in zip(ths[:-1], ths[1:]):\n",
    "#        mask = snn_ratio <= th\n",
    "#        #print (mask.sum())\n",
    "#        AA = is_correct[mask].float().mean()\n",
    "#        #print (mask.sum().item(), AA.item())\n",
    "#        out.append(AA.item())\n",
    "#    return out\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1+0c9e625'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.version.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.version.__version__\n",
    "\n",
    "desc = K.feature.SOSNet(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_descriptor_on_dataset( desc,\n",
    "    ds_name='notredame',\n",
    "    resol=100,\n",
    "    device=torch.device('cuda:0'),\n",
    "    ds_root = '/home/mishkdmy/datasets/Brown'):\n",
    "    \n",
    "    desc.eval()\n",
    "    desc = desc.to(device)\n",
    "    dataset = PhotoTourRevisited(ds_root,\n",
    "                      ds_name,\n",
    "                       train=False, \n",
    "                       download=True)\n",
    "    orig_size = 64\n",
    "    out_size = 32\n",
    "    test_aug = nn.Sequential( \n",
    "        K.Resize((out_size,out_size), interpolation='bicubic'))\n",
    "    BS = 1024\n",
    "    TEST_BS = 128\n",
    "    N_WORKERS = 4\n",
    "    \n",
    "    dl_train = TfmdDL(dataset,\n",
    "                 device=device,\n",
    "                 after_item=[ToTensor], \n",
    "                 after_batch=[TupleAug(test_aug)], #two patches -> single tensor\n",
    "                 bs=BS, num_workers=N_WORKERS,\n",
    "                 shuffle = False)\n",
    "    num_patches = len(dl_train.dataset)\n",
    "    descriptors = []#torch.zeros(num_patches, 128)\n",
    "    all_labels = []#torch.zeros(num_patches)\n",
    "    all_img_labels = []#torch.zeros(num_patches)\n",
    "    Miner = miners.BatchHardMiner()\n",
    "    count = 0 \n",
    "    max_img = -1\n",
    "    min_img = 1000\n",
    "    prec_per_img = []\n",
    "    rec_per_img = []\n",
    "    ths_per_img = []\n",
    "    aps = []\n",
    "    print ('Extracting descriptors and calculating AP')\n",
    "    for patches, labels, img_labels in progress_bar(dl_train):\n",
    "        with torch.no_grad():\n",
    "            descs = desc(patches)\n",
    "            descriptors.append(descs)\n",
    "            all_labels.append(labels)\n",
    "            all_img_labels.append(img_labels)\n",
    "            all_img_labels_cat = torch.cat(all_img_labels)\n",
    "            img_labels_unique = torch.sort(torch.unique(all_img_labels_cat).long())[0]\n",
    "            new_max_img = img_labels_unique.max().item()\n",
    "            new_min_img = img_labels_unique.min().item()\n",
    "            if new_min_img != new_max_img:\n",
    "                all_img_labels = torch.cat(all_img_labels)\n",
    "                descriptors = torch.cat(descriptors)\n",
    "                all_labels = torch.cat(all_labels)\n",
    "                for ii in img_labels_unique[:-1]:\n",
    "                    current_batch = all_img_labels == ii\n",
    "                    cur_descs = descriptors[current_batch].cpu()\n",
    "                    #print (cur_descs.shape)\n",
    "                    cur_labels = all_labels[current_batch].cpu().long()\n",
    "                    NN = cur_labels.size(0)\n",
    "                    pos_matrix = (cur_labels[None].expand(NN,NN) == cur_labels[...,None].expand(NN,NN)) != (torch.eye(NN).to(cur_labels.device)>0)\n",
    "                    #print ('pos_matrix_done')\n",
    "                    neg = torch.zeros(NN).long()\n",
    "                    \n",
    "                    if NN > 2000: # To avoid OOM, we will find hard minimum in batches\n",
    "                        bs1 = 128\n",
    "                        nb = (NN // bs1)  \n",
    "                        for i in range(nb):\n",
    "                            st = i*bs1\n",
    "                            fin = min(NN, (i+1)*bs1)\n",
    "                            if fin == st:\n",
    "                                break\n",
    "                            dm = (torch.cdist(cur_descs[st:fin], cur_descs) +\n",
    "                                  1000.0 * (pos_matrix[st:fin].to(cur_descs.dtype)) + \n",
    "                                1000.0*torch.eye(NN)[st:fin].to(cur_labels.device).to(cur_descs.dtype))\n",
    "                            min_idx = torch.min(dm, dim=1)[1]\n",
    "                            neg[st:fin] = min_idx       \n",
    "                    #anc, pos, neg = Miner(cur_descs, cur_labels)\n",
    "                    pos_idxs = torch.arange(NN)[None].expand(NN,NN)[pos_matrix]\n",
    "                    anc_idxs = torch.nonzero(pos_matrix)[:,0]\n",
    "                    pos_matrix = None\n",
    "                    neg_idxs = neg[anc_idxs]\n",
    "                    pos_dists = F.pairwise_distance(cur_descs[anc_idxs], cur_descs[pos_idxs])\n",
    "                    neg_dists = F.pairwise_distance(cur_descs[anc_idxs], cur_descs[neg_idxs])\n",
    "                    correct = pos_dists <= neg_dists\n",
    "                    snn = torch.min(pos_dists,neg_dists) / torch.max(pos_dists,neg_dists)\n",
    "                    snn[torch.isnan(snn)] = 1.0\n",
    "                    #precision, recall, thresholds = precision_recall_curve(correct, 1-snn)\n",
    "                    ap = average_precision_score(correct, 1-snn)\n",
    "                    #prec_per_img.append(precision)\n",
    "                    #rec_per_img.append(recall)\n",
    "                    #ths_per_img.append(thresholds)\n",
    "                    aps.append(ap)\n",
    "                current_batch = all_img_labels == img_labels_unique[-1].item()\n",
    "                descriptors = [descriptors[current_batch]]\n",
    "                all_img_labels = [all_img_labels[current_batch]]\n",
    "                all_labels = [all_labels[current_batch]]\n",
    "                gc.collect()\n",
    "    all_img_labels = torch.cat(all_img_labels)\n",
    "    descriptors = torch.cat(descriptors)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    for ii in img_labels_unique:\n",
    "        current_batch = all_img_labels == ii\n",
    "        cur_descs = descriptors[current_batch].cpu()\n",
    "        cur_labels = all_labels[current_batch].cpu()\n",
    "        NN = cur_labels.size(0)\n",
    "        \n",
    "        pos_matrix = (cur_labels[None].expand(NN,NN) == cur_labels[...,None].expand(NN,NN)) != (torch.eye(NN).to(cur_labels.device)>0)\n",
    "        #print ('pos_matrix_done')\n",
    "        neg = torch.zeros(NN).long()\n",
    "        if NN > 5000:\n",
    "            bs1 = 128\n",
    "            nb = (NN // bs1)  \n",
    "            for bi in range(nb):\n",
    "                st = bi*bs1\n",
    "                fin = min(NN, (bi+1)*bs1)\n",
    "                if fin == st:\n",
    "                    break\n",
    "                dm = (torch.cdist(cur_descs[st:fin], cur_descs) +\n",
    "                      1000.0 * (pos_matrix[st:fin].to(cur_descs.dtype)) + \n",
    "                      1000.0*torch.eye(NN)[st:fin].to(cur_labels.device).to(cur_descs.dtype))\n",
    "                min_idx = torch.min(dm, dim=1)[1]\n",
    "                neg[st:fin] = min_idx       \n",
    "        pos_idxs = torch.arange(NN)[None].expand(NN,NN)[pos_matrix]\n",
    "        anc_idxs = torch.nonzero(pos_matrix)[:,0]\n",
    "        pos_matrix = None\n",
    "        neg_idxs = neg[anc_idxs]\n",
    "        pos_dists = F.pairwise_distance(cur_descs[anc_idxs], cur_descs[pos_idxs])\n",
    "        neg_dists = F.pairwise_distance(cur_descs[anc_idxs], cur_descs[neg_idxs])\n",
    "        correct = pos_dists <= neg_dists\n",
    "        snn = torch.min(pos_dists,neg_dists) / torch.max(pos_dists,neg_dists)\n",
    "        snn[torch.isnan(snn)] = 1.0\n",
    "        #precision, recall, thresholds = precision_recall_curve(correct, 1-snn)\n",
    "        #prec_per_img.append(precision)\n",
    "        #rec_per_img.append(recall)\n",
    "        #ths_per_img.append(thresholds)\n",
    "        ap = average_precision_score(correct, 1-snn)\n",
    "        aps.append(ap)\n",
    "    descriptors = None\n",
    "    all_labels = None\n",
    "    all_img_labels = None\n",
    "    dataset=None\n",
    "    dl_train = None\n",
    "    gc.collect()\n",
    "    return aps\n",
    "    #return {\"precision\": prec_per_img, \"recall\": rec_per_img, \"thresholds\": ths_per_img}\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "from collections import defaultdict\n",
    "results = defaultdict(dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notredame\n",
      "RootSIFT\n",
      "# Found cached data /home/mishkdmy/datasets/Brown/notredame.pt\n",
      "Extracting descriptors and calculating AP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='458' class='' max='458' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [458/458 04:12<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mishkdmy/.conda/envs/fastai1/lib/python3.7/site-packages/ipykernel_launcher.py:79: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RootSIFT mAP = 0.579\n",
      "SIFT\n",
      "# Found cached data /home/mishkdmy/datasets/Brown/notredame.pt\n",
      "Extracting descriptors and calculating AP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='458' class='' max='458' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [458/458 04:01<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIFT mAP = 0.569\n",
      "HardNet\n",
      "# Found cached data /home/mishkdmy/datasets/Brown/notredame.pt\n",
      "Extracting descriptors and calculating AP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='458' class='' max='458' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [458/458 03:50<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HardNet mAP = 0.686\n",
      "SOSNet\n",
      "# Found cached data /home/mishkdmy/datasets/Brown/notredame.pt\n",
      "Extracting descriptors and calculating AP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='458' class='' max='458' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [458/458 04:14<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOSNet mAP = 0.688\n",
      "yosemite\n",
      "RootSIFT\n",
      "# Found cached data /home/mishkdmy/datasets/Brown/yosemite.pt\n",
      "Extracting descriptors and calculating AP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='619' class='' max='619' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [619/619 04:02<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RootSIFT mAP = 0.682\n",
      "SIFT\n",
      "# Found cached data /home/mishkdmy/datasets/Brown/yosemite.pt\n",
      "Extracting descriptors and calculating AP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='619' class='' max='619' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [619/619 04:41<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIFT mAP = 0.685\n",
      "HardNet\n",
      "# Found cached data /home/mishkdmy/datasets/Brown/yosemite.pt\n",
      "Extracting descriptors and calculating AP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='619' class='' max='619' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [619/619 03:44<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HardNet mAP = 0.777\n",
      "SOSNet\n",
      "# Found cached data /home/mishkdmy/datasets/Brown/yosemite.pt\n",
      "Extracting descriptors and calculating AP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='619' class='' max='619' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [619/619 04:14<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOSNet mAP = 0.776\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "results = defaultdict(dict)\n",
    "for ds_name in val_ds_names:\n",
    "    print (ds_name)\n",
    "    for desc, desc_name in zip([\n",
    "         K.feature.SIFTDescriptor(32, rootsift=True).to(torch.device('cuda:0')),\n",
    "         K.feature.SIFTDescriptor(32, rootsift=False).to(torch.device('cuda:0')),\n",
    "        K.feature.HardNet(True),\n",
    "        K.feature.SOSNet(True)], ['RootSIFT', 'SIFT', 'HardNet', 'SOSNet']):\n",
    "        print (desc_name)\n",
    "        results[ds_name][desc_name] = eval_descriptor_on_dataset(desc,\n",
    "                                                                 ds_name, resol=50)\n",
    "        print (f'{desc_name} mAP = {np.array(results[ds_name][desc_name]).mean():.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
